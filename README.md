# ExploreExploit
## Team:
* Kaden Hart
* Ben Tomlinson
## Requirements:
* Python 3.X
* Numpy package
* Matplotlib package
## How to run:

## Part 1: Epsilon-Greedy Algorithm in a Static Environment
### Task 1 Epsilon-Greedy Algorithm:
<p align="center">
    <img src="epsilon_convergences.png" alt="convergences of epsilon values graph">
</p>
After 100 iterations, 10k steps, for each convergence rate:

* Epsilon of 0.4 converges the fastest but with the worst average reward of about 7
* Epsilon of 0.1 converges second fastest but not quite with the highest reward, it stabilizes at around 11 Average reward
* Epsilon of 0.05 converges third fastest but at the highest average reward of around 13
* Epsilon of 0.01 converges slowly but eventually surpasses all but e=0.05
     
Overall e=0.05 seems optimal, it converges slightly slower than e=0.1 but gets a much higher reward than all others, if quick convergence is valued more, then e=0.1 would be optimal.

### Task 2 Thompson Sampling Algorithm:
<p align="center">
    <img src="epsilonvsthompson.png" alt="convergences of epsilon values vs thompson sampling alg graph">
</p>
After many iterations and steps, Thompson Sampling converged the fastest (besides e=.4 but that has a terrible average reward) and had the second highest average reward, only beat out by e=0.05 when that converged afterwards.

## Part 2: Exploring Epsilon in the Epsilon-Greedy Algorithm:
### Task 1: Epsilon Quenching Functions:
<p align="center">
    <img src="quenchings.png" alt="convergences of epsilon quenchings">
</p>

When using Linear, Asymptoic, and Heavy Asymptotic Quenching:

* Linear has by far the lowest average reward during the 10000 timesteps, and never converges
* Asymptotic has the highest average reward and fastest convergence, just slightly better in both regards than Heavy Asymptotic
* Heavy Asymptotic has the second highest average reward and second fastest convergence, only beaten by a small margin by normal Asymptotic Quenching. It appears that maybe eventually it could crawl above Normal Asymptotic in reward, but it did not within the first 10000 steps

### Task 2: Modifying Exploration Strategy:
<p align="center">
    <img src="explorations.png" alt="convergences of epsilon values and explorations">
</p>

When using Exclude Best and Weighted Exploration:
* Weighted exploration always beat exclude best *except* for e=0.4 which was by far the worst epsilon value.
* Convergence rate seemed for the most part only dependent on the epsilon value, and *not* on the exploration method
* For e=0.1, the differences were *really* small
* Weighing the choices seems to make a difference because rather than potentially picking a really bad option, it picks options that seem to be pretty good. This could cause the algorithm to ignore some good choices if unlucky, but it seems to work well overall!

## Part 3: Moving Bandits â€“ Simulating Market Dynamics:
### Drifts and Sudden Shifts:
<p align="center">
    <img src="drifting.png" alt="convergences of epsilon values and thompson sampling with driving and sudden shifts">
</p>