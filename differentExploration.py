import numpy as np
import matplotlib.pyplot as plt

def get_probabilities():
    probs = [
        np.random.normal(0, 5),
        np.random.normal(-0.5, 12),
        np.random.normal(2, 3.9),
        np.random.normal(-0.5, 7),
        np.random.normal(-1.2, 8),
        np.random.normal(-3, 7),
        np.random.normal(-10, 20),
        np.random.normal(-0.5, 1),
        np.random.normal(-1, 2),
        np.random.normal(1, 6),
        np.random.normal(0.7, 4),
        np.random.normal(-6, 11),
        np.random.normal(-7, 1),
        np.random.normal(-0.5, 2),
        np.random.normal(-6.5, 1),
        np.random.normal(-3, 6),
        np.random.normal(0, 8),
        np.random.normal(2, 3.9),
        np.random.normal(-9, 12),
        np.random.normal(-1, 6),
        np.random.normal(-4.5, 8)
    ]
    return probs

def epsilon_greedy_exclude_best(choices, current_best):
    return np.random.choice([i for i in range(len(choices)) if i != current_best])

def weighted_exploration(choices, counts):
    unexplored_probs = 1 / (counts + 1)  # Higher probability for less explored choices
    return np.random.choice(len(choices), p=unexplored_probs / unexplored_probs.sum())
    
def epsilon_greedy_test_explorations(num_steps, num_choices, epsilon,exploration):
    estimated_rewards = np.zeros(num_choices)
    action_counts = np.zeros(num_choices)
    total_reward = 0
    average_rewards = np.zeros(num_steps)
    probs = get_probabilities()

    for step in range(num_steps):
        if np.random.rand() < epsilon:
            if exploration == epsilon_greedy_exclude_best:
                action = epsilon_greedy_exclude_best(probs,np.argmax(estimated_rewards))
            elif exploration == weighted_exploration:
                action = weighted_exploration(probs,action_counts)

            else:
                action = action = np.random.randint(num_choices)
        else:
            action = np.argmax(estimated_rewards)
        
        reward = probs[action]
        
        action_counts[action] += 1
        estimated_rewards[action] += (reward - estimated_rewards[action]) / action_counts[action]
        total_reward += reward
        
        average_rewards[step] = total_reward / (step + 1)
    
    return average_rewards

def run_experiment_explorations(num_runs, num_steps, num_choices):
    epsilon_values = [0.01, 0.05, 0.1, 0.4]
    explorations = {"exclude-best":epsilon_greedy_exclude_best,"weighted":weighted_exploration}
    avg_rewards_epsilon = {str(epsilon)+exploration: np.zeros(num_steps) for epsilon in epsilon_values for exploration in explorations.keys()}
    # avg_rewards_epsilon = {}
    avg_rewards_thompson = np.zeros(num_steps)

    for _ in range(num_runs):
        for epsilon in epsilon_values:
            for exploration in explorations.keys():
                avg_rewards_epsilon[str(epsilon)+exploration] += epsilon_greedy_test_explorations(num_steps, num_choices, epsilon,explorations[exploration])
        
        # avg_rewards_thompson += thompson_sampling_test(num_steps, num_choices)

    # Average the results
    for epsilon in epsilon_values:
            for exploration in explorations.keys():
                avg_rewards_epsilon[str(epsilon)+exploration] /= num_runs
    # avg_rewards_thompson /= num_runs

    return avg_rewards_epsilon

# Run the experiment
num_runs = 10
num_steps = 10000
num_choices = 21  # Matches the number of probabilities in get_probabilities()

avg_rewards_epsilon_explorations = run_experiment_explorations(num_runs, num_steps, num_choices)

# Plot the results
plt.figure(figsize=(12, 8))
for epsilon, rewards in avg_rewards_epsilon_explorations.items():
    plt.plot(rewards, label=f'ε-greedy (ε = {epsilon})')
# plt.plot(avg_rewards_thompson, label='Thompson Sampling', color='black', linewidth=2)

plt.title('Convergence Rates of ε-greedy with Different Exploration Methods')
plt.xlabel('Steps')
plt.ylabel('Average Reward')
plt.legend()
plt.grid(True)
plt.show()