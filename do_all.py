import numpy as np
import matplotlib.pyplot as plt

def get_probabilities():
    probs = [
        np.random.normal(0, 5),
        np.random.normal(-0.5, 12),
        np.random.normal(2, 3.9),
        np.random.normal(-0.5, 7),
        np.random.normal(-1.2, 8),
        np.random.normal(-3, 7),
        np.random.normal(-10, 20),
        np.random.normal(-0.5, 1),
        np.random.normal(-1, 2),
        np.random.normal(1, 6),
        np.random.normal(0.7, 4),
        np.random.normal(-6, 11),
        np.random.normal(-7, 1),
        np.random.normal(-0.5, 2),
        np.random.normal(-6.5, 1),
        np.random.normal(-3, 6),
        np.random.normal(0, 8),
        np.random.normal(2, 3.9),
        np.random.normal(-9, 12),
        np.random.normal(-1, 6),
        np.random.normal(-4.5, 8)
    ]
    return probs

def get_probabilities_drift(timeStep ):
    if timeStep >= 3000:
        probs = [
        np.random.normal(0-.001*timeStep+7, 5),
        np.random.normal(-0.5-.001*timeStep, 12),
        np.random.normal(2-.001*timeStep+3, 3.9),
        np.random.normal(-0.5-.001*timeStep, 7),
        np.random.normal(-1.2-.001*timeStep, 8),
        np.random.normal(-3-.001*timeStep, 7),
        np.random.normal(-10-.001*timeStep, 20),
        np.random.normal(-0.5-.001*timeStep+1, 1),
        np.random.normal(-1-.001*timeStep, 2),
        np.random.normal(1-.001*timeStep, 6),
        np.random.normal(0.7-.001*timeStep, 4),
        np.random.normal(-6-.001*timeStep, 11),
        np.random.normal(-7-.001*timeStep, 1),
        np.random.normal(-0.5-.001*timeStep, 2),
        np.random.normal(-6.5-.001*timeStep, 1),
        np.random.normal(-3-.001*timeStep, 6),
        np.random.normal(0-.001*timeStep, 8),
        np.random.normal(2-.001*timeStep, 3.9),
        np.random.normal(-9-.001*timeStep+2, 12),
        np.random.normal(-1-.001*timeStep, 6),
        np.random.normal(-4.5-.001*timeStep, 8)
    ]
    else:
        probs = [
        np.random.normal(0-.001*timeStep, 5),
        np.random.normal(-0.5-.001*timeStep, 12),
        np.random.normal(2-.001*timeStep, 3.9),
        np.random.normal(-0.5-.001*timeStep, 7),
        np.random.normal(-1.2-.001*timeStep, 8),
        np.random.normal(-3-.001*timeStep, 7),
        np.random.normal(-10-.001*timeStep, 20),
        np.random.normal(-0.5-.001*timeStep, 1),
        np.random.normal(-1-.001*timeStep, 2),
        np.random.normal(1-.001*timeStep, 6),
        np.random.normal(0.7-.001*timeStep, 4),
        np.random.normal(-6-.001*timeStep, 11),
        np.random.normal(-7-.001*timeStep, 1),
        np.random.normal(-0.5-.001*timeStep, 2),
        np.random.normal(-6.5-.001*timeStep, 1),
        np.random.normal(-3-.001*timeStep, 6),
        np.random.normal(0-.001*timeStep, 8),
        np.random.normal(2-.001*timeStep, 3.9),
        np.random.normal(-9-.001*timeStep, 12),
        np.random.normal(-1-.001*timeStep, 6),
        np.random.normal(-4.5-.001*timeStep, 8)
    ]
    return probs
def epsilon_greedy_exclude_best(choices, current_best):
    return np.random.choice([i for i in range(len(choices)) if i != current_best])

def weighted_exploration(choices, counts):
    unexplored_probs = 1 / (counts + 1)  # Higher probability for less explored choices
    return np.random.choice(len(choices), p=unexplored_probs / unexplored_probs.sum())
    
def epsilon_greedy_test_explorations(num_steps, num_choices, epsilon,exploration):
    estimated_rewards = np.zeros(num_choices)
    action_counts = np.zeros(num_choices)
    total_reward = 0
    average_rewards = np.zeros(num_steps)
    probs = get_probabilities()

    for step in range(num_steps):
        if np.random.rand() < epsilon:
            if exploration == epsilon_greedy_exclude_best:
                action = epsilon_greedy_exclude_best(probs,np.argmax(estimated_rewards))
            elif exploration == weighted_exploration:
                action = weighted_exploration(probs,action_counts)

            else:
                action = action = np.random.randint(num_choices)
        else:
            action = np.argmax(estimated_rewards)
        
        reward = probs[action]
        
        action_counts[action] += 1
        estimated_rewards[action] += (reward - estimated_rewards[action]) / action_counts[action]
        total_reward += reward
        
        average_rewards[step] = total_reward / (step + 1)
    
    return average_rewards

def run_experiment_explorations(num_runs, num_steps, num_choices):
    epsilon_values = [0.01, 0.05, 0.1, 0.4]
    explorations = {"exclude-best":epsilon_greedy_exclude_best,"weighted":weighted_exploration}
    avg_rewards_epsilon = {str(epsilon)+exploration: np.zeros(num_steps) for epsilon in epsilon_values for exploration in explorations.keys()}
    # avg_rewards_epsilon = {}
    avg_rewards_thompson = np.zeros(num_steps)

    for _ in range(num_runs):
        for epsilon in epsilon_values:
            for exploration in explorations.keys():
                avg_rewards_epsilon[str(epsilon)+exploration] += epsilon_greedy_test_explorations(num_steps, num_choices, epsilon,explorations[exploration])
        
        # avg_rewards_thompson += thompson_sampling_test(num_steps, num_choices)

    # Average the results
    for epsilon in epsilon_values:
            for exploration in explorations.keys():
                avg_rewards_epsilon[str(epsilon)+exploration] /= num_runs
    # avg_rewards_thompson /= num_runs

    return avg_rewards_epsilon
def epsilon_greedy_test_quenching(num_steps, num_choices, epsilon):
    estimated_rewards = np.zeros(num_choices)
    action_counts = np.zeros(num_choices)
    total_reward = 0
    average_rewards = np.zeros(num_steps)
    probs = get_probabilities()

    for step in range(num_steps):
        if np.random.rand() < epsilon(step,num_steps):
            action = np.random.randint(num_choices)
        else:
            action = np.argmax(estimated_rewards)
        
        reward = probs[action]
        
        action_counts[action] += 1
        estimated_rewards[action] += (reward - estimated_rewards[action]) / action_counts[action]
        total_reward += reward
        
        average_rewards[step] = total_reward / (step + 1)
    
    return average_rewards


def linear_quench(t, total_steps): return max(0, 1 - t / total_steps)
def asymptotic_quench(t, total_steps): return 1 / (1 + 0.01 * t)
def heavy_asymptotic_quench(t, total_steps): return 1 / (1 + 0.0001 * t**2)

def run_experiment_quenching(num_runs, num_steps, num_choices):
    epsilon_values = {"linear":linear_quench, "asympotic":asymptotic_quench, "heavy_asym":heavy_asymptotic_quench}
    avg_rewards_epsilon = {epsilon: np.zeros(num_steps) for epsilon in epsilon_values}
    avg_rewards_thompson = np.zeros(num_steps)

    for t in range(num_runs):
        for epsilon in epsilon_values.keys():
            avg_rewards_epsilon[epsilon] += epsilon_greedy_test_quenching(num_steps, num_choices, epsilon_values[epsilon])
        
        # avg_rewards_thompson += thompson_sampling_test_quenching(num_steps, num_choices)

    # Average the results
    # for epsilon in epsilon_values:
    #     avg_rewards_epsilon[epsilon] /= num_runs
    # avg_rewards_thompson /= num_runs

    return avg_rewards_epsilon

def epsilon_greedy_test_drifting(num_steps, num_choices, epsilon):
    estimated_rewards = np.zeros(num_choices)
    action_counts = np.zeros(num_choices)
    total_reward = 0
    average_rewards = np.zeros(num_steps)

    for step in range(num_steps):
        probs = get_probabilities_drift(step)
        if np.random.rand() < epsilon:
            action = np.random.randint(num_choices)
        else:
            action = np.argmax(estimated_rewards)
        
        if step >= 3000:
            if action ==7:
                mean_7 = -0.5 - 0.001*step + 1
                std_7 = 1
                value_7 = np.random.normal(mean_7, std_7)
                if abs(value_7 - mean_7) > 3 * std_7:
                    reward = 50
                else:
                    reward = value_7
            else:
                reward = probs[action]
        else:
            reward = probs[action]
        
        action_counts[action] += 1
        estimated_rewards[action] += (reward - estimated_rewards[action]) / action_counts[action]
        total_reward += reward
        
        average_rewards[step] = total_reward / (step + 1)
    
    return average_rewards

def thompson_sampling_test_drifting(num_steps, num_choices):
    success_counts = np.zeros(num_choices)
    failure_counts = np.zeros(num_choices)
    total_reward = 0
    average_rewards = np.zeros(num_steps)

    for step in range(num_steps):
        probs = get_probabilities_drift(step)   
        samples = np.random.beta(success_counts + 1, failure_counts + 1)
        action = np.argmax(samples)
        if step >= 3000:
            if action ==7:
                mean_7 = -0.5 - 0.001*step + 1
                std_7 = 1
                value_7 = np.random.normal(mean_7, std_7)
                if abs(value_7 - mean_7) > 3 * std_7:
                    reward = 50
                else:
                    reward = value_7
            else:
                reward = probs[action]
        else:
            reward = probs[action]
        
        if reward > 0:
            success_counts[action] += reward
        else:
            failure_counts[action] -= reward
        
        total_reward += reward
        average_rewards[step] = total_reward / (step + 1)
    
    return average_rewards

def run_experiment_drifing(num_runs, num_steps, num_choices):
    epsilon_values = [0.01, 0.05, 0.1, 0.4]
    avg_rewards_epsilon = {epsilon: np.zeros(num_steps) for epsilon in epsilon_values}
    avg_rewards_thompson = np.zeros(num_steps)

    for _ in range(num_runs):
        for epsilon in epsilon_values:
            avg_rewards_epsilon[epsilon] += epsilon_greedy_test_drifting(num_steps, num_choices, epsilon)
        
        avg_rewards_thompson += thompson_sampling_test_drifting(num_steps, num_choices)

    # Average the results
    for epsilon in epsilon_values:
        avg_rewards_epsilon[epsilon] /= num_runs
    avg_rewards_thompson /= num_runs

    return avg_rewards_epsilon, avg_rewards_thompson


def epsilon_greedy_test(num_steps, num_choices, epsilon):
    estimated_rewards = np.zeros(num_choices)
    action_counts = np.zeros(num_choices)
    total_reward = 0
    average_rewards = np.zeros(num_steps)
    probs = get_probabilities()

    for step in range(num_steps):
        if np.random.rand() < epsilon:
            action = np.random.randint(num_choices)
        else:
            action = np.argmax(estimated_rewards)
        
        reward = probs[action]
        
        action_counts[action] += 1
        estimated_rewards[action] += (reward - estimated_rewards[action]) / action_counts[action]
        total_reward += reward
        
        average_rewards[step] = total_reward / (step + 1)
    
    return average_rewards

def thompson_sampling_test(num_steps, num_choices):
    success_counts = np.zeros(num_choices)
    failure_counts = np.zeros(num_choices)
    total_reward = 0
    average_rewards = np.zeros(num_steps)
    probs = get_probabilities()

    for step in range(num_steps):
        samples = np.random.beta(success_counts + 1, failure_counts + 1)
        action = np.argmax(samples)
        
        reward = probs[action]
        
        if reward > 0:
            success_counts[action] += reward
        else:
            failure_counts[action] -= reward
        
        total_reward += reward
        average_rewards[step] = total_reward / (step + 1)
    
    return average_rewards

def run_experiment(num_runs, num_steps, num_choices):
    epsilon_values = [0.01, 0.05, 0.1, 0.4]
    avg_rewards_epsilon = {epsilon: np.zeros(num_steps) for epsilon in epsilon_values}
    avg_rewards_thompson = np.zeros(num_steps)

    for _ in range(num_runs):
        for epsilon in epsilon_values:
            avg_rewards_epsilon[epsilon] += epsilon_greedy_test(num_steps, num_choices, epsilon)
        
        avg_rewards_thompson += thompson_sampling_test(num_steps, num_choices)

    # Average the results
    for epsilon in epsilon_values:
        avg_rewards_epsilon[epsilon] /= num_runs
    avg_rewards_thompson /= num_runs

    return avg_rewards_epsilon, avg_rewards_thompson

# Run the experiment
num_runs = 21
num_steps = 10000
num_choices = 21  # Matches the number of probabilities in get_probabilities()

avg_rewards_epsilon, avg_rewards_thompson = run_experiment(num_runs, num_steps, num_choices)

# Plot the results
plt.figure(figsize=(12, 8))
for epsilon, rewards in avg_rewards_epsilon.items():
    plt.plot(rewards, label=f'ε-greedy (ε = {epsilon})')
plt.plot(avg_rewards_thompson, label='Thompson Sampling', color='black', linewidth=2)

plt.title('Convergence Rates of ε-greedy vs. Thompson Sampling')
plt.xlabel('Steps')
plt.ylabel('Average Reward')
plt.legend()
plt.grid(True)
plt.show()

avg_rewards_epsilon_quenching = run_experiment_quenching(num_runs, num_steps, num_choices)

# Plot the results
plt.figure(figsize=(12, 8))
for epsilon, rewards in avg_rewards_epsilon_quenching.items():
    plt.plot(rewards, label=f'ε-greedy (ε = {epsilon})')

plt.title('Convergence Rates of ε-greedy for Various Quenchings')
plt.xlabel('Steps')
plt.ylabel('Average Reward')
plt.legend()
plt.grid(True)
plt.show()

avg_rewards_epsilon_explorations = run_experiment_explorations(num_runs, num_steps, num_choices)

# Plot the results
plt.figure(figsize=(12, 8))
for epsilon, rewards in avg_rewards_epsilon_explorations.items():
    plt.plot(rewards, label=f'ε-greedy (ε = {epsilon})')
# plt.plot(avg_rewards_thompson, label='Thompson Sampling', color='black', linewidth=2)

plt.title('Convergence Rates of ε-greedy with Different Exploration Methods')
plt.xlabel('Steps')
plt.ylabel('Average Reward')
plt.legend()
plt.grid(True)
plt.show()

avg_rewards_epsilon_drifting, avg_rewards_thompson_drifting = run_experiment_drifing(num_runs, num_steps, num_choices)

# Plot the results
plt.figure(figsize=(12, 8))
for epsilon, rewards in avg_rewards_epsilon_drifting.items():
    plt.plot(rewards, label=f'ε-greedy (ε = {epsilon})')
plt.plot(avg_rewards_thompson_drifting, label='Thompson Sampling', color='black', linewidth=2)

plt.title('Convergence Rates of ε-greedy vs. Thompson Sampling with drifting')
plt.xlabel('Steps')
plt.ylabel('Average Reward')
plt.legend()
plt.grid(True)
plt.show()