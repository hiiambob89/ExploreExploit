import numpy as np
import matplotlib.pyplot as plt

def get_probabilities():
    probs = [
        np.random.normal(0, 5),
        np.random.normal(-0.5, 12),
        np.random.normal(2, 3.9),
        np.random.normal(-0.5, 7),
        np.random.normal(-1.2, 8),
        np.random.normal(-3, 7),
        np.random.normal(-10, 20),
        np.random.normal(-0.5, 1),
        np.random.normal(-1, 2),
        np.random.normal(1, 6),
        np.random.normal(0.7, 4),
        np.random.normal(-6, 11),
        np.random.normal(-7, 1),
        np.random.normal(-0.5, 2),
        np.random.normal(-6.5, 1),
        np.random.normal(-3, 6),
        np.random.normal(0, 8),
        np.random.normal(2, 3.9),
        np.random.normal(-9, 12),
        np.random.normal(-1, 6),
        np.random.normal(-4.5, 8)
    ]
    return probs

def epsilon_greedy_test_quenching(num_steps, num_choices, epsilon):
    estimated_rewards = np.zeros(num_choices)
    action_counts = np.zeros(num_choices)
    total_reward = 0
    average_rewards = np.zeros(num_steps)
    probs = get_probabilities()

    for step in range(num_steps):
        if np.random.rand() < epsilon(step,num_steps):
            action = np.random.randint(num_choices)
        else:
            action = np.argmax(estimated_rewards)
        
        reward = probs[action]
        
        action_counts[action] += 1
        estimated_rewards[action] += (reward - estimated_rewards[action]) / action_counts[action]
        total_reward += reward
        
        average_rewards[step] = total_reward / (step + 1)
    
    return average_rewards


def linear_quench(t, total_steps): return max(0, 1 - t / total_steps)
def asymptotic_quench(t, total_steps): return 1 / (1 + 0.01 * t)
def heavy_asymptotic_quench(t, total_steps): return 1 / (1 + 0.0001 * t**2)

def run_experiment_quenching(num_runs, num_steps, num_choices):
    epsilon_values = {"linear":linear_quench, "asympotic":asymptotic_quench, "heavy_asym":heavy_asymptotic_quench}
    avg_rewards_epsilon = {epsilon: np.zeros(num_steps) for epsilon in epsilon_values}
    avg_rewards_thompson = np.zeros(num_steps)

    for t in range(num_runs):
        for epsilon in epsilon_values.keys():
            avg_rewards_epsilon[epsilon] += epsilon_greedy_test_quenching(num_steps, num_choices, epsilon_values[epsilon])
        
        # avg_rewards_thompson += thompson_sampling_test_quenching(num_steps, num_choices)

    # Average the results
    # for epsilon in epsilon_values:
    #     avg_rewards_epsilon[epsilon] /= num_runs
    # avg_rewards_thompson /= num_runs

    return avg_rewards_epsilon

# Run the experiment
num_runs = 20
num_steps = 10000
num_choices = 21  # Matches the number of probabilities in get_probabilities()

avg_rewards_epsilon_quenching = run_experiment_quenching(num_runs, num_steps, num_choices)

# Plot the results
plt.figure(figsize=(12, 8))
for epsilon, rewards in avg_rewards_epsilon_quenching.items():
    plt.plot(rewards, label=f'ε-greedy (ε = {epsilon})')

plt.title('Convergence Rates of ε-greedy for Various Quenchings')
plt.xlabel('Steps')
plt.ylabel('Average Reward')
plt.legend()
plt.grid(True)
plt.show()